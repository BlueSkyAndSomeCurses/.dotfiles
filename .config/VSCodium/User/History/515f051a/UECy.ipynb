{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Опис алгоритму:\n",
    "Спробуйте розібратися з основами реалізації нейронної мережі для вирішення завдання XOR. Для цього вам потрібно самостійіно реалізувати деякі функії:\n",
    "- `sigmoid` - функція активації\n",
    "- `sigmoid_derivative` - похідна функції активації\n",
    "- `forward_propagation` - передбачення моделі\n",
    "- `loss_derivative` - функція втрат\n",
    "- `loss_curve` - функція для візуалізації зміни значення функції втрат.\n",
    "\n",
    "Все інше вам дано, готова основна функція `train`, яка реалізує алгоритм зворотнього поширення помилки.\n",
    "Ви можете змінювати параметри нейронної мережі, щоб покращити результат. Необхідно 2 вхідних нейрони, 1 вихідний, достаньо 1 прихованого шару з 2 нейронами.\n",
    "\n",
    "Для того аби більше дізнатися про нейронні мережі, та сам алгоритм зворотнього поширення помилки, рекомендую переглянути це прекрасне відео: https://www.youtube.com/watch?v=Ilg3gGewQ5U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Набір даних:\n",
    "Використовуйте простий набір даних для виконання завдання XOR. Наприклад, можна створити набір з чотирьох можливих комбінацій входів і відповідних вихідних значень.\n",
    "\n",
    "| Input A | Input B | Output (XOR) |\n",
    "|---------|---------|--------------|\n",
    "|    0    |    0    |      0       |\n",
    "|    0    |    1    |      1       |\n",
    "|    1    |    0    |      1       |\n",
    "|    1    |    1    |      0       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архітектура нейронної мережі:\n",
    "Розгляньте просту архітектуру для вирішення задачі XOR. Нейронна мережа повинна мати вхідний шар, прихований шар і вихідний шар."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_hidden = 2\n",
    "num_outputs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Математика за нейронними мережами:\n",
    "Як вже будо згадано вище, основою всього є алгоритм зворотнього поширення помилки (backpropagation). Основна його ціль - мінімізувати фукцію втрат(loss function). Для цього використовується градієнтний спуск (gradient descent). Про сам градієнт будете вивчати під час другуго семестру).\n",
    "Цей алгоритм використовується для знаходження локального мінімуму функції втрат. Це означає, що ми шукаємо такі значення ваг, при яких функція втрат буде мати найменше значення. Для цього ми використовуємо похідну функції втрат по вагам. Це дозволяє нам змінювати значення вагів так, щоб функція втрат зменшувалась. Цей процес повторюється до тих пір, поки функція втрат не досягне мінімуму.\n",
    "\n",
    "Хороше відео про градієнтний спуск: https://www.youtube.com/watch?v=IHZwWFHWa-w\n",
    "\n",
    "### Ланцюжкове правило (chain rule):\n",
    "Рахувати похідну для кожної змінної окремо - дуже складно. Тому ми використовуємо ланцюжкове правило (chain rule). Грубо кажучи, ми рахуємо похідну функції втрат по кожному шару нейронної мережі, починаючи з останнього. Це дозволяє нам знайти похідну функції втрат по вагам кожного шару. І використовуючи градієнтний спуск, ми змінюємо значення ваг так, щоб функція втрат зменшувалась.\n",
    "\n",
    "Добре пояснення також у вищезгаданому відео: https://www.youtube.com/watch?v=Ilg3gGewQ5U\n",
    "\n",
    "### Функція втрат (loss function):\n",
    "Функція втрат - це функція, яка визначає, наскільки добре працює наша модель. Чим менше значення функції втрат, тим краще працює модель. Для вирішення задачі XOR ми використовуємо квадратичну функцію втрат. Це просто значення квадратів різниць між прогнозованими і фактичними значеннями.\n",
    "\n",
    "Cost functuion: $C = \\frac{1}{2}(\\hat{y} - y)^2$\n",
    "\n",
    "### Функція активації та параметри нейрона (activation function):\n",
    "Як саме мережа буде передавати значення від одного шару до іншого? Як взагалі працює нейрон? Нейрон - це просто функція, яка приймає вхідні значення, виконує деякі обчислення і повертає вихідне значення. Ці обчислення включають в себе додавання вхідних значень, множення на ваги і додавання зміщення (bias). Після цього значення проходить через функцію активації. Це дозволяє нейрону вирішувати нелінійні задачі. \n",
    "\n",
    "Нехай $x_1, ..., x_k$ - вхідні значення, $w_1, ..., w_k$ - ваги, $b$ - зміщення, $f$ - функція активації. Тоді вихід нейрона буде рівним $f(x_1w_1 + ... + x_kw_k + b)$.\n",
    "\n",
    "Хороша стаття про функції активації: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "\n",
    "Існує багато різних функцій активації. Вони використовуються для різних задач. Наприклад, для задач класифікації використовують функцію активації softmax. Для задач регресії - лінійну функцію активації. Для вирішення задач XOR ми використовуємо сигмоїдальну функцію активації. Вона виглядає так: $f(x) = \\frac{1}{1 + e^{-x}}$. До того ж, дана функція хороша тим, що її областю значень є $(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реалізація алгоритму на Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<!-- Do not edit this file with editors other than draw.io -->\n",
    "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
    "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.1\" width=\"432px\" height=\"191px\" viewBox=\"-0.5 -0.5 432 191\" class=\"ge-export-svg-dark\" content=\"&lt;mxfile host=&quot;app.diagrams.net&quot; modified=&quot;2023-11-24T12:49:22.466Z&quot; agent=&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36&quot; etag=&quot;APl0wJugyGrk2QD_2ts9&quot; version=&quot;22.1.3&quot; type=&quot;google&quot;&gt;&lt;diagram name=&quot;Page-1&quot; id=&quot;42789a77-a242-8287-6e28-9cd8cfd52e62&quot;&gt;7Zpbb9owFMc/DY+bkpiQ5LGBtpu0ddUqbfTRJCZYNTEyBkI//Wxi5wos5Va25gn72OckPv+fL3LogP40uWdwNvlOQ0Q6lhEmHTDoWJZp2Ib4kZZ1arFBNzVEDIeqU254wq9IeyrrAodoXurIKSUcz8rGgMYxCnjJBhmjq3K3MSXlp85ghGqGpwCSuvU3DvlEWU3DyBu+IBxN1KNdPeARDF4iRhexel5MY5S2TKEOo7rOJzCkq4IJ3HZAn1HK09I06SMi06ozlvrd7WjNXpmhmDdyQBCYjuMZoRVAzwWfnDTCEpKFSoN6Ub7WeRERhASi4q8mmKOnGQxky0pQIGwTPiWiZoriJgVIPskQtWyosimgUxyoBgJHiPhZzvqUUCaaNlkTbpzRl0wA6TumMb+DU0wkV78QC2EMlVlBZIo8+pDgKBaVQKQCiYB+PTcqXUvEOEoKJpWre0SniLO16KJabe2ikDYdpeMqB8SxU9OkgEbPU1gqJKMscq6NKCh5GkrltlLtlcooSQW6biOpHOsMUnmtVHukAhWp3nVW6d2n1aqRVqDrNZtW5jm0Mlut9mjVc8taWb1m80pPv2O0in48Pnx7cPreKBnewq9LDqyfzeZWHN7Iw5vMBYHzuUz8bplQgvlQtcjys7R/tlVtkBS6Dda6EouxDIuVgpes5m6bmvZLXxWFtVNjTaI5XbBA99qxF3DIIsR39HG261rQTZ81i7ppG0MEcrwsv+Y2MdUTHikWA8h3TrM6xSs8pMNTXsXDZTVQr8KfVwmU5qAWaMNWNuwjcGuyPByP2wGwmZdBTQvZsnZ+1qxLsGYesrBdz7LmtqidAjXQ7qLNlraWt5Pw1r1i3i5Dm9MANnPHYbyl7W202ddL24WObe7Hoa1XDWRcmLbe32mbT+BMFscEJQo7/40Enn2v3MuK956oZHH0eqK3k7eiAsxqoAuj0uDTyD+PimlcEyvAOZCVd9/EGnybOQErB2xsJ9qi/pPtx62uKZ59UU70ZXqJkx4R2fJHohDJwrO2iCdkxhpNHCW8zErtRlleHOMAkhvVMMVhKN19hub4FY42oSQjMzncTQJsv2MPZKwFp3N1W53dcleuvseYkIpJDa56Yb0Ft5132I5rlxXSCBVIA1tIA8ZuqI66w7a2XSpWJBuaH1ozq1ueVdu+53UvqpnVavZGzQA4m2aimv/BJV1J8z8Qgds/&lt;/diagram&gt;&lt;/mxfile&gt;\"><defs><style type=\"text/css\">svg.ge-export-svg-dark &gt; * { filter: invert(100%) hue-rotate(180deg); }&#xa;svg.ge-export-svg-dark image { filter: invert(100%) hue-rotate(180deg) }</style></defs><g><ellipse cx=\"238.88\" cy=\"25.88\" rx=\"28.125\" ry=\"25.875\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.75\" pointer-events=\"all\" transform=\"translate(1.5,2.25)\" opacity=\"0.25\"/><ellipse cx=\"238.88\" cy=\"25.88\" rx=\"28.125\" ry=\"25.875\" fill=\"rgb(255, 255, 255)\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" pointer-events=\"all\"/><ellipse cx=\"238.13\" cy=\"160.5\" rx=\"28.125\" ry=\"27\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.75\" pointer-events=\"all\" transform=\"translate(1.5,2.25)\" opacity=\"0.25\"/><ellipse cx=\"238.13\" cy=\"160.5\" rx=\"28.125\" ry=\"27\" fill=\"rgb(255, 255, 255)\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" pointer-events=\"all\"/><ellipse cx=\"88.13\" cy=\"25.88\" rx=\"28.125\" ry=\"25.875\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.75\" pointer-events=\"all\" transform=\"translate(1.5,2.25)\" opacity=\"0.25\"/><ellipse cx=\"88.13\" cy=\"25.88\" rx=\"28.125\" ry=\"25.875\" fill=\"rgb(255, 255, 255)\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" pointer-events=\"all\"/><ellipse cx=\"88.13\" cy=\"160.88\" rx=\"28.125\" ry=\"26.625\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.75\" pointer-events=\"all\" transform=\"translate(1.5,2.25)\" opacity=\"0.25\"/><ellipse cx=\"88.13\" cy=\"160.88\" rx=\"28.125\" ry=\"26.625\" fill=\"rgb(255, 255, 255)\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" pointer-events=\"all\"/><ellipse cx=\"358.13\" cy=\"93.75\" rx=\"28.125\" ry=\"26.25\" fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.75\" pointer-events=\"all\" transform=\"translate(1.5,2.25)\" opacity=\"0.25\"/><ellipse cx=\"358.13\" cy=\"93.75\" rx=\"28.125\" ry=\"26.25\" fill=\"rgb(255, 255, 255)\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" pointer-events=\"all\"/><path d=\"M 116.25 25.88 L 205.97 25.88\" fill=\"none\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"stroke\"/><path d=\"M 209.91 25.88 L 204.66 28.5 L 205.97 25.88 L 204.66 23.25 Z\" fill=\"rgb(0, 0, 0)\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"all\"/><path d=\"M 108.02 142.06 L 215.39 47.32\" fill=\"none\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"stroke\"/><path d=\"M 218.35 44.72 L 216.15 50.16 L 215.39 47.32 L 212.67 46.22 Z\" fill=\"rgb(0, 0, 0)\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"all\"/><path d=\"M 108.02 44.16 L 214.79 138.13\" fill=\"none\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"stroke\"/><path d=\"M 217.74 140.73 L 212.07 139.23 L 214.79 138.13 L 215.54 135.29 Z\" fill=\"rgb(0, 0, 0)\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"all\"/><path d=\"M 116.25 160.88 L 205.22 160.52\" fill=\"none\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"stroke\"/><path d=\"M 209.16 160.5 L 203.92 163.15 L 205.22 160.52 L 203.9 157.9 Z\" fill=\"rgb(0, 0, 0)\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"all\"/><path d=\"M 267 25.88 L 334.44 72.35\" fill=\"none\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"stroke\"/><path d=\"M 337.68 74.59 L 331.87 73.77 L 334.44 72.35 L 334.85 69.45 Z\" fill=\"rgb(0, 0, 0)\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"all\"/><path d=\"M 266.25 160.5 L 334.4 115.08\" fill=\"none\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"stroke\"/><path d=\"M 337.67 112.9 L 334.76 118 L 334.4 115.08 L 331.85 113.63 Z\" fill=\"rgb(0, 0, 0)\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"all\"/><path d=\"M 8.38 36.67 L 7.37 29.23 L 45 24.13 L 43.94 16.32 L 59.63 25.93 L 47.07 39.36 L 46.01 31.56 Z\" fill=\"none\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"all\"/><path d=\"M 7.31 156.76 L 8.43 149.35 L 46.09 155 L 47.26 147.21 L 59.63 160.82 L 43.81 170.2 L 44.98 162.41 Z\" fill=\"none\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"all\"/><path d=\"M 386.63 97.5 L 386.63 90 L 412.88 90 L 412.88 82.13 L 427.13 93.75 L 412.88 105.38 L 412.88 97.5 Z\" fill=\"none\" stroke=\"rgb(0, 0, 0)\" stroke-width=\"0.75\" stroke-miterlimit=\"10\" pointer-events=\"all\"/><rect x=\"408.75\" y=\"60\" width=\"22.5\" height=\"22.5\" fill=\"none\" stroke=\"none\" pointer-events=\"all\"/><g transform=\"translate(-0.5 -0.5)scale(0.75)\"><switch><foreignObject pointer-events=\"none\" width=\"134%\" height=\"134%\" requiredFeatures=\"http://www.w3.org/TR/SVG11/feature#Extensibility\" style=\"overflow: visible; text-align: left;\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 95px; margin-left: 560px;\"><div data-drawio-colors=\"color: rgb(0, 0, 0); \" style=\"box-sizing: border-box; font-size: 0px; text-align: center;\"><div style=\"display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: nowrap;\"><b>Y</b></div></div></div></foreignObject><text x=\"560\" y=\"99\" fill=\"rgb(0, 0, 0)\" font-family=\"Helvetica\" font-size=\"12px\" text-anchor=\"middle\">Y</text></switch></g><rect x=\"0\" y=\"0\" width=\"30\" height=\"22.5\" fill=\"none\" stroke=\"none\" pointer-events=\"all\"/><g transform=\"translate(-0.5 -0.5)scale(0.75)\"><switch><foreignObject pointer-events=\"none\" width=\"134%\" height=\"134%\" requiredFeatures=\"http://www.w3.org/TR/SVG11/feature#Extensibility\" style=\"overflow: visible; text-align: left;\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 15px; margin-left: 20px;\"><div data-drawio-colors=\"color: rgb(0, 0, 0); \" style=\"box-sizing: border-box; font-size: 0px; text-align: center;\"><div style=\"display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: nowrap;\"><b>X1</b></div></div></div></foreignObject><text x=\"20\" y=\"19\" fill=\"rgb(0, 0, 0)\" font-family=\"Helvetica\" font-size=\"12px\" text-anchor=\"middle\">X1</text></switch></g><rect x=\"0\" y=\"120\" width=\"30\" height=\"22.5\" fill=\"none\" stroke=\"none\" pointer-events=\"all\"/><g transform=\"translate(-0.5 -0.5)scale(0.75)\"><switch><foreignObject pointer-events=\"none\" width=\"134%\" height=\"134%\" requiredFeatures=\"http://www.w3.org/TR/SVG11/feature#Extensibility\" style=\"overflow: visible; text-align: left;\"><div xmlns=\"http://www.w3.org/1999/xhtml\" style=\"display: flex; align-items: unsafe center; justify-content: unsafe center; width: 1px; height: 1px; padding-top: 175px; margin-left: 20px;\"><div data-drawio-colors=\"color: rgb(0, 0, 0); \" style=\"box-sizing: border-box; font-size: 0px; text-align: center;\"><div style=\"display: inline-block; font-size: 12px; font-family: Helvetica; color: rgb(0, 0, 0); line-height: 1.2; pointer-events: all; white-space: nowrap;\"><b>X1</b></div></div></div></foreignObject><text x=\"20\" y=\"179\" fill=\"rgb(0, 0, 0)\" font-family=\"Helvetica\" font-size=\"12px\" text-anchor=\"middle\">X1</text></switch></g></g><switch><g requiredFeatures=\"http://www.w3.org/TR/SVG11/feature#Extensibility\"/><a transform=\"translate(0,-5)\" xlink:href=\"https://www.drawio.com/doc/faq/svg-export-text-problems\" target=\"_blank\"><text text-anchor=\"middle\" font-size=\"10px\" x=\"50%\" y=\"100%\">Text is not SVG - cannot display</text></a></switch></svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основна функція, яку вам потрібно реалізувати - `forward`. Вона приймає на вхід вхідні дані, ваги і зміщення, і повертає значення вихідного шару. Як вона працює?\n",
    "\n",
    "Маємо два вхідних значення $x_1$ та $x_2$. Вони подаються на вхід першого шару нейронної мережі. Далі вони множаться на ваги $w_1$ та $w_2$ відповідно. Після цього додається зміщення $b$. І вихідні значення подаються на функцію активації. Це і буде значенням першого шару нейронної мережі. Далі ці значення подаються на вхід другого шару. І вже з другого шару ми отримуємо значення вихідного шару. Це значення і буде значенням, яке повертає функція `forward`.\n",
    "\n",
    "Тобто, *нейрон просто приймає зважену суму вхідних значень, додає до неї зміщення і подає на функцію активації*.\n",
    "\n",
    "$a_{out} = f(x_1w_1 + x_2w_2 + b)$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**! Зважену суму можна записати у вигляді скалярного добутку матриць.** Тоді отримаємо:\n",
    "\n",
    "Оскільки ніхто не обраховує вивід для кожного нейрона окремо, а використовує векторизацію, то ми можемо записати вхідний шар у вигляді матриці. Тоді вхідний шар буде мати вигляд: $\\vec{x} = [x_1, x_2]$. Ваги також можна записати у вигляді матриці. Тоді, наприклад, для вхідного шару будуть мати вигляд: $\\vec{w} = [[w_{11}, w_{12}], [w_{21}, w_{22}]]$. \n",
    "\n",
    "Не будемо використовувати зміщення, адже в нашому випадку воно практично ніяк не впилватиме на результат.\n",
    "\n",
    "Тоді,\n",
    "- $l_{in} = f(\\vec{x} \\cdot \\vec{w_{in}})$ - результат з вхідного шару\n",
    "- $l_{out} = f(\\vec{l_{in}} \\cdot \\vec{w_{out}})$ - результат з вихідного шару\n",
    "\n",
    "Скалярний добуток реалізує функція `np.dot`. Функцію активації реалізує функція `sigmoid`(яку вам потрібно реалізувати самостійно).\n",
    "\n",
    "Хоч наша мережа дуже простенька, функцію `forward` вартує імплементовувати використовуючи бібліотеку `numpy`. Це дозволить нам використовувати векторизацію, яка надає зручність та швидкість обчислень."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "\n",
    "def sigmoid_derivative(sigmoided_x):\n",
    "    return sigmoided_x * (1-sigmoided_x)\n",
    "\n",
    "def cost_derivative(y, y_hat):\n",
    "    return y_hat - y \n",
    "    # return 0.5 * ((y - y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**! Зверніть увагу на те, що в коді у функцію `sigmoid_derivative`, передається не вихід функції активації, а вхід - результат із функції `sigmoid`**\n",
    "\n",
    "Тобто, `sigmoid_derivative` приймає на вхід $f(x)$. Тому знайдену у знайденій похідній заміняємо  $\\frac{1}{1 + e^{-x}}$ на `sigmoided_x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size) # weights for input -> hidden\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size) # weights for hidden -> output\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, inputs):\n",
    "        l_in = sigmoid(np.dot(inputs, self.weights_input_hidden))\n",
    "        return sigmoid(np.dot(l_in, self.weights_hidden_output))\n",
    "    \n",
    "    # Тут за вас уже імплементовано backpropagation\n",
    "    def train(self, inputs, targets, epochs, learning_rate = 0.01):\n",
    "        loss_history = [] # keep track of loss over epochs\n",
    "        \n",
    "        for _ in range(epochs): # train for a fixed number of epochs\n",
    "            hidden_input = np.dot(inputs, self.weights_input_hidden) # weighted sum\n",
    "            hidden_output = sigmoid(hidden_input) # activation function\n",
    "            output_input = np.dot(hidden_output, self.weights_hidden_output)\n",
    "            output = sigmoid(output_input)\n",
    "\n",
    "            # Обрахуйте похідну функції втрат в точці 'output'\n",
    "            error = cost_derivative(output, targets)\n",
    "\n",
    "            d_output =  error * sigmoid_derivative(output)\n",
    "            error_hidden = d_output.dot(self.weights_hidden_output.T) # backpropagate error\n",
    "            d_hidden = error_hidden * sigmoid_derivative(hidden_output) # compute gradients\n",
    "            \n",
    "            self.weights_hidden_output -= learning_rate * hidden_output.T.dot(d_output) # update weights\n",
    "            self.weights_input_hidden -= learning_rate*inputs.T.dot(d_hidden)\n",
    "\n",
    "            loss = np.mean(np.square(error)) # compute loss\n",
    "            loss_history.append(loss) # keep track of loss over epochs\n",
    "            \n",
    "        return loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Що вийшло?\n",
    "Перевірте роботу вашої нейронної мережі. Якщо все виконано правильно, то ви повинні отримати такі значення близькі до 0 та 1.\n",
    "\n",
    "Приклад виводу:\n",
    "```\n",
    "Final loss: 0.0048\n",
    "\n",
    "Output after training:\n",
    "[[0.03265805]\n",
    " [0.93100316]\n",
    " [0.93100315]\n",
    " [0.09200636]]\n",
    "\n",
    "Expected output:\n",
    "[[0 1 1 0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2544706366470009, 0.25623090154205264, 0.2586130971331125, 0.26180224073119696, 0.2660111144925884, 0.2714640528760142, 0.2783654722983713, 0.286852169394814, 0.2969365489632148, 0.30845933709235357, 0.3210775624737778, 0.33430594657142987, 0.3476048903924245, 0.3604814547543346, 0.37256361968539065, 0.3836274085326843, 0.39358283802852934, 0.40243864863033246, 0.41026430760028076, 0.4171593599866264, 0.42323270886328945, 0.42859045240869087, 0.43332975860461653, 0.4375364910095272, 0.44128493528639234, 0.4446385751990962, 0.447651303867998, 0.4503687376352738, 0.4528294676889124, 0.4550661788349608, 0.4571066147956613, 0.4589743940798161, 0.4606896912551521, 0.46226980195731815, 0.4637296098861341, 0.46508197236270593, 0.4663380387909889, 0.4675075140978589, 0.46859887714756754, 0.4696195623235129, 0.47057611095369173, 0.47147429800404783, 0.47231923844182266, 0.4731154768423813, 0.4738670631437497, 0.47457761691358136, 0.47525038205844516, 0.47588827355464813, 0.47649391749657605, 0.4770696855293424, 0.4776177245466129, 0.47813998238328936, 0.4786382301094222, 0.4791140814308648, 0.47956900961944626, 0.4800043623273383, 0.4804213745840956, 0.48082118022831255, 0.4812048219871837, 0.4815732603850702, 0.4819273816352707, 0.4822680046466562, 0.4825958872578904, 0.4829117317959939, 0.48321619004251937, 0.4835098676791839, 0.4837933282750825, 0.4840670968693497, 0.4843316631960647, 0.4845874845921534, 0.484834988623856, 0.48507457546286514, 0.48530662003939407, 0.4855314739961147, 0.4857494674640284, 0.4859609106788355, 0.48616609545419964, 0.4863652965264196, 0.48655877278336146, 0.4867467683890708, 0.48692951381421457, 0.48710722678139684, 0.487280113133415, 0.4874483676316663, 0.48761217469115326, 0.4877717090578701, 0.487927136433759, 0.488078614053898, 0.48822629122011496, 0.4883703097948125, 0.48851080465841257, 0.4886479041335041, 0.48878173037848227, 0.48891239975320333, 0.48904002315894657, 0.4891647063547596, 0.48928655025207773, 0.4894056511893345, 0.48952210118812967, 0.4896359881923815, 0.4897473962917651, 0.4898564059306275, 0.4899630941034681, 0.4900675345379791, 0.49016979786656034, 0.490269951787145, 0.4903680612141049, 0.49046418841994016, 0.4905583931684052, 0.49065073283966465, 0.4907412625480354, 0.49083003525281466, 0.49091710186266985, 0.49100251133401784, 0.49108631076379333, 0.4911685454769796, 0.4912492591092381, 0.491328493684958, 0.4914062896910183, 0.4914826861465312, 0.4915577206688262, 0.4916314295359025, 0.4917038477455732, 0.4917750090715005, 0.49184494611631363, 0.49191369036198285, 0.49198127221761445, 0.49204772106482075, 0.4921130653008037, 0.4921773323792904, 0.49224054884944, 0.49230274039284094, 0.49236393185870575, 0.49242414729736617, 0.49248340999216234, 0.49254174248981536, 0.49259916662936853, 0.492655703569772, 0.4927113738161881, 0.4927661972450833, 0.49282019312817305, 0.49287338015527926, 0.4929257764561592, 0.492977399621357, 0.4930282667221315, 0.49307839432950457, 0.4931277985324774, 0.49317649495545535, 0.4932244987749228, 0.49327182473540304, 0.49331848716474136, 0.49336449998874166, 0.49340987674519243, 0.49345463059730654, 0.49349877434660666, 0.49354232044528357, 0.4935852810080482, 0.49362766782350553, 0.49366949236507096, 0.4937107658014502, 0.4937514990067047, 0.49379170256991833, 0.49383138680448735, 0.4938705617570482, 0.4939092372160609, 0.49394742272006187, 0.49398512756560264, 0.49402236081488704, 0.49405913130312223, 0.49409544764559143, 0.4941313182444672, 0.4941667512953697, 0.4942017547936862, 0.49423633654065846, 0.49427050414924956, 0.49430426504979885, 0.49433762649547497, 0.494370595567533, 0.49440317918038684, 0.49443538408650134, 0.4944672168811146, 0.4944986840067948, 0.4945297917578399, 0.49456054628452506, 0.4945909535972063, 0.49462101957028254, 0.4946507499460253, 0.4946801503382783, 0.4947092262360342, 0.49473798300689203, 0.49476642590040026, 0.4947945600512895, 0.4948223904825997, 0.4948499221087063, 0.4948771597382473, 0.4949041080769565, 0.4949307717304061, 0.4949571552066629, 0.49498326291885764, 0.49500909918767694, 0.4950346682437745, 0.4950599742301075, 0.49508502120420217, 0.4951098131403474, 0.4951343539317221, 0.4951586473924572, 0.4951826972596358, 0.49520650719523107, 0.4952300807879877, 0.49525342155524665, 0.495276532944714, 0.4952994183361802, 0.4953220810431854, 0.4953445243146387, 0.49536675133638863, 0.4953887652327478, 0.49541056906797487, 0.4954321658477122, 0.495453558520384, 0.49547474997855256, 0.4954957430602383, 0.49551654055020056, 0.49553714518118447, 0.49555755963513026, 0.4955777865443515, 0.4955978284926794, 0.49561768801657563, 0.4956373676062146, 0.4956568697065365, 0.4956761967182717, 0.49569535099893625, 0.495714334863803, 0.49573315058684364, 0.4957518004016482, 0.4957702865023181, 0.49578861104433725, 0.4958067761454188, 0.4958247838863304, 0.4958426363116977, 0.49586033543078734, 0.49587788321826853, 0.49589528161495644, 0.4959125325285356, 0.495929637834265, 0.49594659937566515, 0.49596341896518814, 0.49598009838486995, 0.4959966393869675, 0.4960130436945785, 0.4960293130022475, 0.49604544897655417, 0.4960614532566908, 0.49607732745502187, 0.4960930731576322, 0.4961086919248614, 0.4961241852918246, 0.4961395547689206, 0.4961548018423291, 0.4961699279744939, 0.49618493460459695, 0.49619982314901834, 0.4962145950017878, 0.4962292515350243, 0.4962437940993658, 0.4962582240243881, 0.49627254261901477, 0.49628675117191745, 0.49630085095190657, 0.49631484320831276, 0.4963287291713603, 0.49634251005253177, 0.4963561870449231, 0.4963697613235931, 0.4963832340459025, 0.49639660635184696, 0.49640987936438163, 0.49642305418973953, 0.4964361319177415, 0.49644911362210065, 0.4964620003607184, 0.49647479317597576, 0.49648749309501716, 0.49650010113002785, 0.4965126182785068, 0.4965250455235307, 0.4965373838340159, 0.49654963416497194, 0.49656179745775075, 0.49657387464029035, 0.49658586662735316, 0.4965977743207597, 0.4966095986096166, 0.49662134037054073, 0.49663300046787784, 0.4966445797539166, 0.49665607906909903, 0.4966674992422252, 0.4966788410906555, 0.4966901054205063, 0.4967012930268445, 0.49671240469387457, 0.4967234411951261, 0.49673440329363255, 0.49674529174211124, 0.4967561072831359, 0.4967668506493078, 0.49677752256342267, 0.4967881237386348, 0.496798654878617, 0.4968091166777186, 0.4968195098211186, 0.496829834984978, 0.4968400928365868, 0.4968502840345103, 0.4968604092287304, 0.49687046906078564, 0.4968804641639084, 0.496890395163159, 0.4969002626755565, 0.49691006731020915, 0.4969198096684396, 0.49692949034391043, 0.4969391099227447, 0.496948668983646, 0.49695816809801574, 0.49696760783006844, 0.4969769887369437, 0.4969863113688175, 0.4969955762690105, 0.4970047839740952, 0.49701393501400004, 0.49702302991211217, 0.4970320691853782, 0.4970410533444035, 0.49704998289354885, 0.4970588583310256, 0.4970676801489901, 0.49707644883363433, 0.49708516486527754, 0.4970938287184535, 0.4971024408619979, 0.49711100175913425, 0.49711951186755715, 0.49712797163951505, 0.49713638152189066, 0.49714474195628067, 0.49715305337907434, 0.497161316221529, 0.49716953090984617, 0.4971776978652457, 0.49718581750403806, 0.49719389023769534, 0.4972019164729225, 0.4972098966117259, 0.49721783105148015, 0.4972257201849961, 0.4972335644005855, 0.49724136408212594, 0.49724911960912255, 0.49725683135677223, 0.4972644996960233, 0.4972721249936362, 0.4972797076122416, 0.49728724791039997, 0.49729474624265724, 0.4973022029596016, 0.49730961840791843, 0.49731699293044485, 0.4973243268662223, 0.4973316205505499, 0.4973388743150356, 0.4973460884876465, 0.49735326339276004, 0.497360399351211, 0.497367496680342, 0.49737455569404965, 0.49738157670283123, 0.49738856001383147, 0.49739550593088694, 0.49740241475457103, 0.49740928678223717, 0.497416122308063, 0.49742292162309104, 0.4974296850152722, 0.4974364127695051, 0.497443105167678, 0.4974497624887072, 0.4974563850085774, 0.497462973000379, 0.4974695267343465, 0.4974760464778962, 0.4974825324956625, 0.49748898504953354, 0.4974954043986878, 0.4975017907996283, 0.4975081445062176, 0.4975144657697112, 0.4975207548387919, 0.497527011959601, 0.4975332373757732, 0.4975394313284658, 0.4975455940563921, 0.49755172579585183, 0.49755782678076066, 0.4975638972426819, 0.49756993741085476, 0.4975759475122238, 0.4975819277714679, 0.49758787841102803, 0.49759379965113576, 0.4975996917098399, 0.4976055548030338, 0.4976113891444825, 0.497617194945848, 0.4976229724167157, 0.4976287217646197, 0.49763444319506767, 0.49764013691156617, 0.4976458031156441, 0.4976514420068776, 0.4976570537829127, 0.4976626386394897, 0.49766819677046503, 0.497673728367835, 0.49767923362175726, 0.4976847127205729, 0.4976901658508286, 0.4976955931972974, 0.49770099494300035, 0.49770637126922646, 0.497711722355554, 0.49771704837987, 0.49772234951839084, 0.497727625945681, 0.497732877834673, 0.4977381053566864, 0.49774330868144645, 0.49774848797710225, 0.49775364341024597, 0.49775877514593025, 0.49776388334768573, 0.4977689681775388, 0.4977740297960294, 0.49777906836222724, 0.4977840840337492, 0.4977890769667752, 0.4977940473160658, 0.49779899523497706, 0.49780392087547687, 0.4978088243881612, 0.49781370592226826, 0.49781856562569576, 0.4978234036450136, 0.497828220125481, 0.4978330152110594, 0.49783778904442777, 0.49784254176699705, 0.49784727351892377, 0.49785198443912393, 0.497856674665287, 0.49786134433388957, 0.4978659935802079, 0.49787062253833214, 0.49787523134117884, 0.4978798201205034, 0.49788438900691345, 0.49788893812988094, 0.49789346761775444, 0.49789797759777127, 0.49790246819607015, 0.49790693953770226, 0.4979113917466429, 0.49791582494580416, 0.49792023925704487, 0.49792463480118276, 0.49792901169800563, 0.4979333700662818, 0.49793771002377096, 0.4979420316872355, 0.4979463351724508, 0.4979506205942148, 0.49795488806635957, 0.4979591377017609, 0.4979633696123481, 0.49796758390911444, 0.4979717807021263, 0.4979759601005334, 0.49798012221257815, 0.49798426714560495, 0.4979883950060695, 0.49799250589954847, 0.4979965999307474, 0.4980006772035113, 0.49800473782083216, 0.4980087818848581, 0.4980128094969025, 0.4980168207574514, 0.49802081576617374, 0.4980247946219273, 0.4980287574227691, 0.49803270426596213, 0.49803663524798425, 0.49804055046453544, 0.49804445001054587, 0.4980483339801839, 0.4980522024668632, 0.4980560555632507, 0.4980598933612742, 0.49806371595212917, 0.4980675234262867, 0.4980713158734996, 0.4980750933828111, 0.4980788560425605, 0.4980826039403907, 0.49808633716325523, 0.4980900557974245, 0.49809375992849303, 0.4980974496413854, 0.49810112502036397, 0.4981047861490338, 0.49810843311035025, 0.4981120659866248, 0.49811568485953134, 0.4981192898101125, 0.49812288091878537, 0.49812645826534796, 0.49813002192898487, 0.4981335719882733, 0.498137108521189, 0.49814063160511163, 0.49814414131683077, 0.49814763773255144, 0.49815112092790004, 0.4981545909779289, 0.4981580479571226, 0.49816149193940285, 0.49816492299813453, 0.4981683412061295, 0.4981717466356538, 0.4981751393584307, 0.49817851944564706, 0.4981818869679584, 0.498185241995493, 0.49818858459785736, 0.4981919148441414, 0.4981952328029222, 0.49819853854227014, 0.49820183212975244, 0.4982051136324379, 0.4982083831169025, 0.49821164064923246, 0.4982148862950302, 0.49821812011941746, 0.4982213421870409, 0.4982245525620752, 0.4982277513082286, 0.498230938488746, 0.49823411416641405, 0.49823727840356496, 0.49824043126208034, 0.4982435728033957, 0.4982467030885046, 0.4982498221779616, 0.4982529301318873, 0.4982560270099723, 0.49825911287147995, 0.4982621877752511, 0.4982652517797074, 0.49826830494285546, 0.49827134732229006, 0.4982743789751982, 0.4982773999583625, 0.4982804103281647, 0.4982834101405894, 0.4982863994512274, 0.49828937831527914, 0.49829234678755846, 0.49829530492249563, 0.4982982527741404, 0.4983011903961662, 0.4983041178418731, 0.4983070351641901, 0.49830994241568005, 0.4983128396485411, 0.49831572691461123, 0.49831860426537056, 0.4983214717519451, 0.4983243294251083, 0.4983271773352866, 0.4983300155325595, 0.4983328440666652, 0.4983356629870017, 0.4983384723426302, 0.49834127218227847, 0.4983440625543431, 0.4983468435068923, 0.49834961508766906, 0.4983523773440939, 0.498355130323267, 0.49835787407197163, 0.49836060863667625, 0.4983633340635378, 0.4983660503984034, 0.49836875768681405, 0.49837145597400606, 0.4983741453049143, 0.49837682572417485, 0.4983794972761264, 0.49838216000481445, 0.49838481395399215, 0.49838745916712357, 0.498390095687386, 0.49839272355767184, 0.4983953428205915, 0.49839795351847554, 0.4984005556933768, 0.4984031493870731, 0.49840573464106885, 0.4984083114965979, 0.4984108799946252, 0.4984134401758497, 0.49841599208070575, 0.49841853574936557, 0.49842107122174145, 0.49842359853748786, 0.4984261177360034, 0.49842862885643296, 0.4984311319376696, 0.4984336270183568, 0.49843611413689026, 0.4984385933314202, 0.49844106463985277, 0.4984435280998526, 0.4984459837488442, 0.49844843162401453, 0.4984508717623141, 0.4984533042004592, 0.4984557289749343, 0.4984581461219927, 0.49846055567765946, 0.4984629576777326, 0.4984653521577853, 0.49846773915316667, 0.49847011869900515, 0.4984724908302089, 0.4984748555814681, 0.49847721298725606, 0.49847956308183233, 0.4984819058992421, 0.4984842414733204, 0.49848656983769174, 0.49848889102577276, 0.4984912050707736, 0.4984935120056991, 0.49849581186335135, 0.49849810467633, 0.4985003904770353, 0.49850266929766796, 0.4985049411702317, 0.4985072061265351, 0.49850946419819187, 0.49851171541662365, 0.49851395981306035, 0.4985161974185423, 0.4985184282639217, 0.4985206523798634, 0.49852286979684735, 0.4985250805451687, 0.49852728465494056, 0.49852948215609444, 0.4985316730783817, 0.4985338574513756, 0.4985360353044717, 0.49853820666688975, 0.4985403715676748, 0.4985425300356988, 0.49854468209966163, 0.49854682778809223, 0.4985489671293502, 0.49855110015162696, 0.4985532268829468, 0.49855534735116847, 0.49855746158398623, 0.49855956960893066, 0.49856167145337094, 0.4985637671445146, 0.49856585670940984, 0.49856794017494654, 0.49857001756785657, 0.498572088914716, 0.4985741542419459, 0.49857621357581317, 0.4985782669424318, 0.4985803143677642, 0.49858235587762206, 0.49858439149766787, 0.49858642125341485, 0.49858844517022993, 0.4985904632733329, 0.49859247558779846, 0.49859448213855745, 0.4985964829503971, 0.49859847804796265, 0.49860046745575815, 0.49860245119814783, 0.49860442929935633, 0.4986064017834705, 0.4986083686744398, 0.49861032999607807, 0.498612285772063, 0.49861423602593885, 0.49861618078111636, 0.49861812006087364, 0.49862005388835773, 0.49862198228658516, 0.49862390527844275, 0.4986258228866887, 0.4986277351339534, 0.49862964204274074, 0.49863154363542783, 0.49863343993426773, 0.49863533096138857, 0.49863721673879524, 0.4986390972883705, 0.4986409726318751, 0.49864284279094917, 0.49864470778711273, 0.49864656764176674, 0.4986484223761941, 0.49865027201155987, 0.49865211656891284, 0.4986539560691856, 0.49865579053319553, 0.49865761998164626, 0.4986594444351276, 0.49866126391411647, 0.49866307843897856, 0.49866488802996733, 0.4986666927072265, 0.49866849249079026, 0.4986702874005833, 0.4986720774564225, 0.4986738626780171, 0.4986756430849699, 0.4986774186967776, 0.49867918953283147, 0.4986809556124182, 0.49868271695472055, 0.49868447357881873, 0.49868622550368946, 0.49868797274820864, 0.4986897153311503, 0.4986914532711886, 0.49869318658689765, 0.4986949152967525, 0.49869663941912973, 0.4986983589723083, 0.4987000739744699, 0.49870178444370017, 0.4987034903979879, 0.49870519185522766, 0.4987068888332192, 0.4987085813496679, 0.49871026942218655, 0.4987119530682942, 0.4987136323054189, 0.4987153071508964, 0.498716977621972, 0.4987186437358002, 0.4987203055094466, 0.49872196295988686, 0.4987236161040085, 0.49872526495861075, 0.49872690954040605, 0.49872854986601933, 0.49873018595198965, 0.4987318178147704, 0.4987334454707294, 0.49873506893615044, 0.49873668822723294, 0.49873830336009256, 0.4987399143507626, 0.49874152121519333, 0.4987431239692536, 0.4987447226287305, 0.4987463172093305, 0.49874790772667954, 0.4987494941963238, 0.49875107663373003, 0.49875265505428656, 0.49875422947330306, 0.49875579990601127, 0.49875736636756596, 0.49875892887304485, 0.4987604874374494, 0.4987620420757052, 0.49876359280266225, 0.4987651396330959, 0.4987666825817071, 0.49876822166312273, 0.49876975689189607, 0.4987712882825075, 0.49877281584936467, 0.4987743396068035, 0.49877585956908754, 0.4987773757504098, 0.49877888816489213, 0.49878039682658615, 0.49878190174947357, 0.49878340294746665, 0.4987849004344085, 0.4987863942240737, 0.49878788433016885, 0.49878937076633223, 0.49879085354613556, 0.4987923326830829, 0.4987938081906121, 0.49879528008209506, 0.49879674837083776, 0.498798213070081, 0.4987996741930004, 0.49880113175270757, 0.49880258576224956, 0.49880403623461, 0.49880548318270923, 0.4988069266194043, 0.49880836655749006, 0.498809803009699, 0.49881123598870186, 0.49881266550710796, 0.4988140915774657, 0.49881551421226256, 0.49881693342392586, 0.49881834922482304, 0.4988197616272615, 0.49882117064348996, 0.49882257628569837, 0.4988239785660174, 0.49882537749652006, 0.4988267730892218, 0.49882816535607993, 0.49882955430899534, 0.4988309399598113, 0.4988323223203155, 0.49883370140223904, 0.4988350772172572, 0.49883644977699015, 0.4988378190930023, 0.49883918517680415, 0.49884054803985106, 0.49884190769354464, 0.498843264149232, 0.4988446174182076, 0.49884596751171273, 0.4988473144409345, 0.49884865821700897, 0.4988499988510192, 0.4988513363539965, 0.49885267073692025, 0.4988540020107188, 0.49885533018626926, 0.49885665527439815, 0.4988579772858816, 0.4988592962314451, 0.4988606121217648, 0.498861924967467, 0.49886323477912903, 0.4988645415672787, 0.4988658453423954, 0.49886714611491023, 0.4988684438952057, 0.49886973869361695, 0.498871030520431, 0.49887231938588805, 0.4988736053001809, 0.4988748882734556, 0.4988761683158117, 0.4988774454373025, 0.49887871964793573, 0.49887999095767244, 0.498881259376429, 0.4988825249140765, 0.49888378758044055, 0.4988850473853028, 0.4988863043384002, 0.4988875584494249, 0.49888880972802613, 0.4988900581838088, 0.4988913038263344, 0.4988925466651215, 0.49889378670964557, 0.4988950239693392, 0.49889625845359287, 0.49889749017175467, 0.49889871913313033, 0.4988999453469847, 0.4989011688225401, 0.4989023895689783, 0.49890360759543967, 0.49890482291102395, 0.49890603552478985, 0.4989072454457565, 0.4989084526829022, 0.49890965724516523, 0.49891085914144484, 0.4989120583806003, 0.4989132549714519, 0.49891444892278025, 0.49891564024332813, 0.4989168289417989, 0.4989180150268579, 0.49891919850713207, 0.4989203793912105, 0.4989215576876445, 0.49892273340494786, 0.49892390655159696, 0.49892507713603085, 0.49892624516665207, 0.49892741065182555, 0.4989285735998811, 0.4989297340191106, 0.4989308919177711, 0.49893204730408275, 0.49893320018623044, 0.4989343505723634, 0.4989354984705955, 0.49893664388900516, 0.4989377868356365, 0.49893892731849804, 0.4989400653455641, 0.4989412009247749, 0.49894233406403543, 0.49894346477121765, 0.4989445930541593, 0.4989457189206642, 0.4989468423785029, 0.4989479634354126, 0.4989490820990975, 0.4989501983772283, 0.4989513122774436, 0.49895242380734894, 0.4989535329745176, 0.49895463978649035, 0.49895574425077616, 0.4989568463748519, 0.49895794616616257, 0.4989590436321222, 0.4989601387801126, 0.49896123161748446, 0.49896232215155806, 0.4989634103896218, 0.49896449633893425, 0.4989655800067227, 0.49896666140018414, 0.4989677405264853, 0.4989688173927631, 0.4989698920061242, 0.4989709643736456, 0.49897203450237415, 0.498973102399328, 0.4989741680714954, 0.4989752315258356, 0.4989762927692789, 0.49897735180872665, 0.4989784086510515, 0.498979463303097, 0.49898051577167934, 0.49898156606358557, 0.49898261418557494, 0.4989836601443782, 0.49898470394669936, 0.4989857455992135, 0.49898678510856875, 0.49898782248138607, 0.4989888577242585, 0.4989898908437523, 0.49899092184640675, 0.49899195073873437, 0.4989929775272205, 0.49899400221832446, 0.4989950248184787, 0.4989960453340897, 0.4989970637715374, 0.49899808013717595, 0.4989990944373335, 0.4990001066783125, 0.4990011168663896, 0.49900212500781627, 0.49900313110881805, 0.49900413517559594, 0.4990051372143247, 0.4990061372311554, 0.4990071352322135, 0.4990081312235996, 0.49900912521139, 0.49901011720163646, 0.49901110720036623, 0.49901209521358236, 0.4990130812472638, 0.4990140653073655]\n",
      "Final loss: 0.4990\n",
      "\n",
      "Output after training:\n",
      "[[0.5        0.5       ]\n",
      " [0.74002514 0.66319321]\n",
      " [0.78661464 0.67012934]\n",
      " [0.91299287 0.80000523]]\n",
      "[[0.99805029]\n",
      " [0.99984564]\n",
      " [0.99989073]\n",
      " [0.99997788]]\n",
      "\n",
      "Expected output:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000 # Спробуйте різні значення та порівняйте результати\n",
    "\n",
    "nn = NeuralNetwork(num_inputs, num_hidden, num_outputs)\n",
    "loss_history = nn.train(data, labels, epochs)\n",
    "\n",
    "print(loss_history)\n",
    "print(\"Final loss: {:.4f}\\n\".format(loss_history[-1]))\n",
    "\n",
    "print(\"Output after training:\")\n",
    "print(nn.forward(data))\n",
    "\n",
    "print(\"\\nExpected output:\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Графік кривої втрат:\n",
    "\n",
    "Спробуйте візуалізувати процесс навчання нейронної мережі. Для цього використовуйте бібліотеку `matplotlib`. Побудуйте графік залежності значення функції втрат від кількості ітерацій навчання(епох).\n",
    "Сама бібліотека `matplotlib` для візуалізації даних, яка вам ще не одноразово знадобиться, тому рекомендую ознайомитися з нею більш детально."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curve(losses):\n",
    "    ...\n",
    "\n",
    "# Your code to plot the loss curve here\n",
    "plot_loss_curve(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Завдання максимально просте, його основна мета познайомити вас з основами нейронних мереж. Тому не лінуйтесь відкрити посилання, які я вам давав, і дізнаєтесь більше про те, як працюють нейронні мережі.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
